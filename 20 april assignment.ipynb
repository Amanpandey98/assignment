{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026bec77-c01f-4f1d-9844-fc837aea9880",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680b485-d314-4909-8c02-fc168921a2c4",
   "metadata": {},
   "source": [
    "The K-nearest neighbors (KNN) algorithm is a non-parametric machine learning algorithm that predicts the class or value of a new data point based on the majority vote or averaging of its k-nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1eb4d0-5e40-4aea-aae8-2c7e9cf0be30",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511c29c-8377-4950-b3c5-eb152a02117f",
   "metadata": {},
   "source": [
    "1.Rule of thumb: A common rule of thumb is to take the square root of the total number of data points in the training set as the value of K. For example, if you have 100 data points, you may choose K = √100 = 10.\n",
    "2.Grid search: Define a range of potential values for K and perform a grid search, evaluating the algorithm performance for each value of K. Choose the value of K that results in the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddfb70-5054-476c-acf5-b1f6207f6aef",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93510a5-a00e-47b3-8b3d-26249c4d9eb4",
   "metadata": {},
   "source": [
    "the KNN classifier is used for discrete classification tasks, where the output is a class label, while the KNN regressor is used for continuous regression tasks, where the output is a numeric or continuos value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7452115-1394-4824-ba05-54ccf70f14b0",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdfa88b-e1df-42f5-8f1b-d688a6ff61eb",
   "metadata": {},
   "source": [
    "To measure the performance of the K-nearest neighbors (KNN) algorithm, various evaluation metrics can be used depending on the specific task, such as classification or regression. Here are some commonly used metrics:\n",
    "\n",
    "1. Classification Metrics:\n",
    "   - Accuracy: The proportion of correctly classified instances to the total number of instances.\n",
    "   - Precision: The ratio of true positives to the sum of true positives and false positives. It measures the accuracy of positive predictions.\n",
    "   - Recall (Sensitivity): The ratio of true positives to the sum of true positives and false negatives. It measures the algorithm's ability to correctly identify positive instances.\n",
    "   - F1 Score: The harmonic mean of precision and recall, providing a balanced measure between the two.\n",
    "   - Area Under the Receiver Operating Characteristic curve (AUC-ROC): It evaluates the performance of the classifier across various thresholds and is particularly useful for imbalanced datasets.\n",
    "\n",
    "2. Regression Metrics:\n",
    "   - Mean Squared Error (MSE): The average of the squared differences between the predicted and actual values. It penalizes larger errors more than smaller ones.\n",
    "   - Root Mean Squared Error (RMSE): The square root of the MSE, providing a more interpretable metric in the same unit as the target variable.\n",
    "   - Mean Absolute Error (MAE): The average of the absolute differences between the predicted and actual values. It provides a more robust measure against outliers compared to MSE.\n",
    "   - R-squared (R²) coefficient: It indicates the proportion of the variance in the target variable that can be explained by the model. It ranges from 0 to 1, where 1 represents a perfect fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee25c8-ac4e-4266-ad58-06d8ec359ec1",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac713f7-c5d7-4c6d-8f44-91ede4678eac",
   "metadata": {},
   "source": [
    "\n",
    "The curse of dimensionality is a phenomenon that occurs in various machine learning algorithms, including the K-nearest neighbors (KNN) algorithm, when the number of features or dimensions in the dataset becomes large. It refers to the challenges and difficulties that arise when working with high-dimensional data.\n",
    "\n",
    "In the context of KNN, the curse of dimensionality can have the following implications:\n",
    "\n",
    "* Increased computational complexity: As the number of dimensions increases, the distance calculation between data points becomes more computationally expensive. KNN requires calculating distances between data points, and with high-dimensional data, the number of distance calculations increases significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae44164-ef4f-4478-a62d-4e932c3f8cb9",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468ef87-6588-494b-989a-dbab191f9c54",
   "metadata": {},
   "source": [
    "* Removing instances: One straightforward approach is to remove instances with missing values from the dataset. However, this approach may lead to a loss of valuable information if the removed instances contain important patterns or relationships. It is only suitable when the missing values are limited and do not significantly affect the dataset's representativeness.\n",
    "\n",
    "* Imputation: Imputation involves estimating or filling in the missing values with reasonable substitutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0408344-9518-4026-8f97-f8f447aa402f",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fe502-cbb2-46ad-8a3a-54b29b8e98a5",
   "metadata": {},
   "source": [
    "The performance of the K-nearest neighbors (KNN) classifier and regressor can vary based on the problem type and the characteristics of the dataset. Here's a comparison between the two and their suitability for different problem types:\n",
    "\n",
    "1. KNN Classifier:\n",
    "   - Suitable for: Classification problems where the target variable consists of discrete or categorical class labels.\n",
    "   - Output: The KNN classifier assigns a class label to each data point.\n",
    "   - Evaluation: Classification performance is typically measured using metrics like accuracy, precision, recall, F1 score, or AUC-ROC.\n",
    "   - Advantages: KNN classifier is intuitive, easy to understand, and performs well when decision boundaries are well-defined and the dataset is not too large.\n",
    "   - Considerations: It may struggle with imbalanced datasets, noisy data, or datasets with a large number of features. It can be computationally expensive with large datasets due to the need for distance calculations.\n",
    "\n",
    "2. KNN Regressor:\n",
    "   - Suitable for: Regression problems where the target variable is continuous or numeric.\n",
    "   - Output: The KNN regressor predicts a continuous value for each data point.\n",
    "   - Evaluation: Regression performance is typically measured using metrics like mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), or R-squared (R²) coefficient.\n",
    "   - Advantages: KNN regressor is useful when there are no strong assumptions about the underlying data distribution, and it can capture nonlinear relationships between features and the target variable.\n",
    "   - Considerations: It can be sensitive to outliers, and the performance may degrade if the feature space is high-dimensional or sparse. The choice of K is crucial, as a smaller K can lead to overfitting, while a larger K can result in underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95750fe-8418-4065-8d1c-b6bfa5c22447",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111692a-7583-4694-9a26-98444ef72a90",
   "metadata": {},
   "source": [
    "Strengths of KNN:\n",
    "\n",
    "- Intuitive and simplicity: KNN is easy to understand and implement, making it a good choice for beginners or for quick prototyping.\n",
    "\n",
    "- Non-parametric: KNN makes no assumptions about the underlying data distribution, allowing it to handle complex relationships and adapt to various types of data.\n",
    "\n",
    "- Flexibility: KNN can handle both classification and regression tasks, making it versatile in different problem domains.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "\n",
    "- Computational complexity: KNN can be computationally expensive, especially with large datasets, as it requires calculating distances between data points during prediction. This can be addressed by using efficient data structures, such as kd-trees or ball trees, to speed up the search for nearest neighbors.\n",
    "\n",
    "- Sensitivity to feature scale: KNN's distance calculations are sensitive to the scale of features. Features with larger magnitudes can dominate the distance calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f8259-0aec-4dd8-a4d4-23879fe18ac6",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42125d0e-3004-431e-bac0-58250b531edd",
   "metadata": {},
   "source": [
    "Euclidean Distance:\n",
    "\n",
    "- Definition: Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates.\n",
    "\n",
    "- Formula: For two points (x₁, y₁) and (x₂, y₂) in a 2D space, the Euclidean distance is given by √((x₂ - x₁)² + (y₂ - y₁)²).\n",
    "\n",
    "Manhattan Distance:\n",
    "\n",
    "- Definition: Manhattan distance, also known as city block distance or L1 distance, measures the sum of absolute differences between corresponding coordinates of two points.\n",
    "\n",
    "- Formula: For two points (x₁, y₁) and (x₂, y₂) in a 2D space, the Manhattan distance is given by |x₂ - x₁| + |y₂ - y₁|.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c255c6-f00d-430e-a0ce-5391acfbe0d7",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc38733-918f-463e-9799-02bf53e24484",
   "metadata": {},
   "source": [
    "Feature scaling plays an important role in the K-nearest neighbors (KNN) algorithm. The primary purpose of feature scaling is to bring different features onto a similar scale, which can significantly impact the performance of KNN. Here's the role of feature scaling in KNN:\n",
    "\n",
    "Distance Calculation: KNN relies on distance calculations between data points to determine their similarity. Features with larger magnitudes can dominate the distance calculations, making KNN biased towards those features. Scaling the features ensures that each feature contributes proportionately to the distance calculation, preventing the dominance of any particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89175ca-00f1-48b7-b0c1-ff86a4bda844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
