{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9128e5-f687-4c06-927f-54ffddc334cd",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c871d26-ce33-4bc7-b53f-3b8e69ccb435",
   "metadata": {},
   "source": [
    "In this problem, we want to find the probability of an employee being a smoker given that he/she uses the health insurance plan. Let's define the events:\n",
    "\n",
    "A = employee is a smoker\n",
    "B = employee uses the health insurance plan\n",
    "\n",
    "Using the information given in the problem, we have:\n",
    "\n",
    "P(B) = 0.7 (70% of employees use the health insurance plan)\n",
    "P(A|B) = ? (what we want to find)\n",
    "P(B|A) = 0.4 (40% of employees who use the plan are smokers)\n",
    "P(A) = ? (we don't know this yet)\n",
    "\n",
    "To find P(A), we need more information. Let's assume that the percentage of smokers among all employees is 20%. Then, we have:\n",
    "\n",
    "P(A) = 0.2 (20% of employees are smokers)\n",
    "\n",
    "Now, we can apply Bayes' theorem:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "P(A|B) = 0.4 * 0.2 / 0.7\n",
    "P(A|B) = 0.114\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.114 or approximately 11.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a28ee-30ab-410c-8b2f-bd5f46820f3b",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483266bf-ce09-414b-a072-0acf1caa1434",
   "metadata": {},
   "source": [
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two commonly used variants of Naive Bayes classification algorithm.\n",
    "\n",
    "The main difference between them lies in the type of data they are best suited for.\n",
    "\n",
    "Bernoulli Naive Bayes is typically used for binary data, where each feature can take on only one of two possible values, typically 0 or 1. It is particularly useful for text classification, where the presence or absence of a particular word in a document can be represented as a binary feature.\n",
    "\n",
    "On the other hand, Multinomial Naive Bayes is typically used for discrete count data, where each feature represents the count of a particular word or feature in a given document. This makes it a good choice for text classification tasks where the frequency of occurrence of words matters more than just their presence or absence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a9339-1c86-410f-8297-0722f6c3a9eb",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac4175-1b63-4fa3-8c9d-5a391f00ade3",
   "metadata": {},
   "source": [
    "\n",
    "Bernoulli Naive Bayes assumes that the input data is binary and that each feature takes on one of two possible values, typically 0 or 1. In the case of missing values, we need to decide how to represent them.\n",
    "\n",
    "One approach is to assign a default value to the missing values, such as 0 or 1. This assumes that the missing values do not carry any particular meaning and are treated the same as the present values. This approach can work well if the missing values are randomly distributed across the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481f152-4d78-4dcb-bf0c-b5e9f949607a",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909ffbd-c5f1-42c9-b567-6d902bd44fe6",
   "metadata": {},
   "source": [
    " Gaussian Naive Bayes can be used for multi-class classification by modeling the likelihood of the features as a multivariate Gaussian distribution for each class and computing the posterior probabilities using Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4217329-943a-41f7-bd31-ca75412bc0ca",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca20d163-9b5e-4133-b3da-2699ddd2881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8282608695652174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.98      0.84       615\n",
      "           1       0.97      0.71      0.82       765\n",
      "\n",
      "    accuracy                           0.83      1380\n",
      "   macro avg       0.85      0.84      0.83      1380\n",
      "weighted avg       0.86      0.83      0.83      1380\n",
      "\n",
      "0.8050724637681159\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84       815\n",
      "           1       0.77      0.76      0.76       565\n",
      "\n",
      "    accuracy                           0.81      1380\n",
      "   macro avg       0.80      0.80      0.80      1380\n",
      "weighted avg       0.80      0.81      0.80      1380\n",
      "\n",
      "0.8797101449275362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90       884\n",
      "           1       0.80      0.90      0.84       496\n",
      "\n",
      "    accuracy                           0.88      1380\n",
      "   macro avg       0.87      0.88      0.87      1380\n",
      "weighted avg       0.89      0.88      0.88      1380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing the necessary dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "##load the dataset\n",
    "df=pd.read_csv('spambase.data.csv')\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "##training the dataset\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "gnb1=GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "parameters1={'priors':[None], 'var_smoothing':[1e-09]}\n",
    "grid1=GridSearchCV(gnb1,param_grid=parameters1,cv=10)\n",
    "grid1.fit(X_train,y_train)\n",
    "y_pred1=grid1.predict(X_test)\n",
    "print(accuracy_score(y_pred1,y_test))\n",
    "print(classification_report(y_pred1,y_test))\n",
    "gnb2=MultinomialNB()\n",
    "parameters2={'alpha':[1.0],'force_alpha':['warn'], 'fit_prior':[True], 'class_prior':[None]}\n",
    "grid2=GridSearchCV(gnb2,param_grid=parameters2,cv=10)\n",
    "grid2.fit(X_train,y_train)\n",
    "y_pred2=grid2.predict(X_test)\n",
    "print(accuracy_score(y_pred2,y_test))\n",
    "print(classification_report(y_pred2,y_test))\n",
    "gnb3=BernoulliNB()\n",
    "gnb3.fit(X_train,y_train)\n",
    "parameter3={'alpha':[1.0],'force_alpha':['warn'],'binarize':[0.0],'fit_prior':[True],'class_prior':[None]}\n",
    "grid3=GridSearchCV(gnb3,param_grid=parameter3,cv=10)\n",
    "grid3.fit(X_train,y_train)\n",
    "y_pred3=grid3.predict(X_test)\n",
    "print(accuracy_score(y_pred3,y_test))\n",
    "print(classification_report(y_pred3,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab357b-3c0b-46b0-86ec-81502a76276f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b88a75-5e80-4791-997c-28fe246d987c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333c355-cf1a-4dd2-b66f-8601378e8ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca415a-78f3-449f-bece-04bf728d45fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
