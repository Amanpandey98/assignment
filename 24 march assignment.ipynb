{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c4cb17-360d-432a-95cf-d37bf2872fe9",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e7608-7374-4bf3-a858-16fdf62c2b0a",
   "metadata": {},
   "source": [
    "Fixed acidity: This refers to the amount of non-volatile acid in wine. It is an important factor in determining the overall acidity of wine.\n",
    "\n",
    "1.Volatile acidity: This refers to the amount of volatile acids, such as acetic acid, present in wine. Higher levels of volatile acidity can lead to unpleasant flavors and aromas in wine.\n",
    "\n",
    "2.Citric acid: This is a weak organic acid that can contribute to the overall acidity of wine. It also adds a citrusy flavor to wine.\n",
    "\n",
    "3.Residual sugar: This is the amount of sugar left over after fermentation. It can contribute to the sweetness of wine.\n",
    "\n",
    "4.Chlorides: This refers to the amount of salt present in wine. Higher levels of chlorides can lead to a salty taste in wine.\n",
    "\n",
    "5.Free sulfur dioxide: This is the amount of sulfur dioxide that is added to wine to prevent oxidation and microbial growth. It can affect the aroma and flavor of wine.\n",
    "\n",
    "6.Total sulfur dioxide: This is the total amount of sulfur dioxide present in wine, including the free and bound forms. It can affect the aroma and flavor of wine.\n",
    "\n",
    "7.Density: This refers to the mass of wine per unit volume. It can provide information about the alcohol content and sugar content of wine.\n",
    "\n",
    "8.pH: This is a measure of the acidity or basicity of wine. It can affect the flavor and stability of wine.\n",
    "\n",
    "9.Sulphates: This refers to the amount of sulfur-containing compounds that are added to wine as a preservative. It can affect the aroma and flavor of wine.\n",
    "\n",
    "10.Alcohol: This is the percentage of alcohol present in wine. It can affect the taste, aroma, and body of wine.\n",
    "\n",
    "11.Quality: This is the overall sensory quality of wine, rated on a scale from 0 (very bad) to 10 (very excellent). It is the target variable for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbb6d5-8d9b-4682-b34d-91a47d188155",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e1ea4-7cd2-4dbb-b0f7-143adecff196",
   "metadata": {},
   "source": [
    "Mean/median imputation: This involves replacing missing values with the mean or median of the non-missing values for that feature. This technique is simple and quick, but it assumes that the missing values are missing at random and can lead to biased results.\n",
    "\n",
    "Regression imputation: This involves using a regression model to predict the missing values based on the non-missing values for that feature. This technique can be more accurate than mean/median imputation, but it can be computationally expensive and may require a large amount of data.\n",
    "\n",
    "K-nearest neighbors imputation: This involves using the values of the k-nearest neighbors to predict the missing values. This technique can be useful when the missing values are clustered together, but it can be sensitive to the choice of k.\n",
    "\n",
    "Multiple imputation: This involves creating multiple imputed datasets, each with different imputed values, and combining the results to obtain a single estimate. This technique can produce more accurate results than single imputation methods, but it can be computationally expensive and may require more data.\n",
    "\n",
    "Each of these techniques has its advantages and disadvantages. Mean/median imputation is simple and quick, but it can lead to biased results. Regression imputation can be more accurate, but it can be computationally expensive and may require more data. K-nearest neighbors imputation can be useful when the missing values are clustered together, but it can be sensitive to the choice of k. Multiple imputation can produce more accurate results, but it can be computationally expensive and may require more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735bce4-ab40-43e7-8ef1-de8a27c83360",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13f765-491c-41f1-bdc2-7366256185fa",
   "metadata": {},
   "source": [
    "lunch,parental level of education,gender are the key factor in student's performace datset that affect student's performance in exams.\n",
    "DATA CHECKS TO PERFORM\n",
    "\n",
    "1.check missing values\n",
    "\n",
    "2.check duplicates\n",
    "\n",
    "3.check datatype\n",
    "\n",
    "4.check the number of unique values\n",
    "\n",
    "5.check statistics of dataset\n",
    "\n",
    "6.check various categories present in the different categorical column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133de66-b037-4789-b373-26ae7a4278ea",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555ee58-0d52-4570-a820-ee3c0c5b8273",
   "metadata": {},
   "source": [
    "Feature engineering is the process of selecting and transforming variables in a dataset to create new features that are more informative for a given machine learning task. In the context of the student performance dataset, feature engineering might involve selecting variables that are likely to be predictive of student performance and transforming them in ways that make them more informative.\n",
    "\n",
    "Here are some possible steps that could be taken in the feature engineering process for the student performance dataset:\n",
    "\n",
    "Select relevant variables: Start by selecting variables that are likely to be relevant to student performance, such as prior academic performance, study habits, attendance, and demographic factors.\n",
    "\n",
    "Encode categorical variables: If any categorical variables are present in the dataset, they should be encoded as numerical variables using techniques such as one-hot encoding or label encoding.\n",
    "\n",
    "Create new features: Create new features that may be more informative for predicting student performance. For example, we could create a new feature that represents the ratio of time spent studying to the number of extracurricular activities a student is involved in.\n",
    "\n",
    "Normalize/Scale features: Normalize or scale the features as needed to ensure that they are on similar scales and have similar distributions.\n",
    "\n",
    "Handle missing values: Determine if any features have missing values, and decide on an appropriate method for imputing missing values.\n",
    "\n",
    "Feature selection: Use techniques such as correlation analysis or feature importance rankings to select the most informative features for the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e50ddc-e49b-4d1b-8cdb-fd7a168351d6",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae005ae-fea8-474c-b2b9-e1c261662548",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing the necessary library\n",
    "import pandas as pd\n",
    "df=pd.read_csv('winequality-red.csv')\n",
    "##checking missing values\n",
    "df.isnull().sum()\n",
    "##checking duplicate data\n",
    "df.duplicated().sum()\n",
    "##checking datatype\n",
    "df.info()\n",
    "##checking the number of unique values\n",
    "df.nunique()\n",
    "##checking the statistics of data \n",
    "df.describe()\n",
    "# Create Q-Q plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "stats.probplot(wine['total sulfur dioxide'], plot=axes[0])\n",
    "axes[0].set_title('Total sulfur dioxide')\n",
    "\n",
    "stats.probplot(wine['free sulfur dioxide'], plot=axes[1])\n",
    "axes[1].set_title('Free sulfur dioxide')\n",
    "\n",
    "plt.tight_layout()\n",
    "From the Q-Q plots, we can see that the distributions of \"total sulfur dioxide\" and \"free sulfur dioxide\" are not normal. Specifically, the tails of the distributions deviate from the normal line.\n",
    "\n",
    "To improve the normality of these features, we can apply various transformations, such as log transformation, power transformation, or rank-based transformation. In this case, a log transformation might be appropriate, as the distributions are skewed to the right.\n",
    "# Apply log transformation to the two features\n",
    "wine['total sulfur dioxide'] = np.log(wine['total sulfur dioxide'])\n",
    "wine['free sulfur dioxide'] = np.log(wine['free sulfur dioxide'])\n",
    "After applying the log transformation, we can create new histograms and box plots to visualize the transformed features.\n",
    "# Create new histograms\n",
    "wine.hist(figsize=(10,10), bins=20)\n",
    "\n",
    "# Create new box plots\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(data=wine, orient='h')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd21ae-8a78-4fee-ac2c-0868b27d2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3047ec-10e3-4dfe-9c7d-e3151d39f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "First, we need to standardize the features in the wine quality data set to have a mean of zero and a standard deviation of one, as PCA is sensitive to the scale of the features.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the wine quality dataset\n",
    "wine = pd.read_csv('winequality.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = wine.iloc[:, :-1]\n",
    "y = wine.iloc[:, -1]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "##Next, we perform PCA on the standardized features and calculate the explained variance ratio for each principal component.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_std)\n",
    "\n",
    "# Calculate the explained variance ratio for each principal component\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "##the explained variance ratio measures the proportion of the total variance in the data that is explained by each principal component. We can plot the cumulative explained variance ratio to determine the minimum number of principal components required to explain 90% of the variance in the data.\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_var_ratio = np.cumsum(explained_var_ratio)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.plot(range(1, len(cumulative_var_ratio)+1), cumulative_var_ratio)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.show()\n",
    "##The plot shows that the first six principal components explain more than 90% of the variance in the data. Therefore, we can reduce the number of features in the wine quality data set from 11 to 6 by selecting the six principal components with the highest variance.\n",
    "# Transform the standardized features using the first six principal components\n",
    "pca = PCA(n_components=6)\n",
    "X_pca = pca.fit_transform(X_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1501d5-bdae-4a1d-b485-4a67f5764254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
