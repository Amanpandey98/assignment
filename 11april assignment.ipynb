{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68851906-b0f2-4f8a-86f3-2c3fef1368ae",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c959c7-56d0-42da-8937-bf383fd52611",
   "metadata": {},
   "source": [
    "Ensemble technique in machine learning involves combining multiple individual models to improve the overall performance of a predictive model. The idea behind ensemble methods is that a group of models can perform better than any single model because the models can complement each other's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921d6b6-71e5-4def-8b95-cf118c53412b",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16284dd1-ee1e-4143-a7ab-6ab44b01b399",
   "metadata": {},
   "source": [
    "Improved accuracy: Ensemble techniques often result in higher accuracy and better generalization performance than individual models. This is because by combining multiple models, ensemble techniques can overcome the limitations of individual models and capture more complex patterns in the data.\n",
    "\n",
    "Robustness: Ensemble techniques are more robust to noise and outliers in the data than individual models. This is because the ensemble can filter out the noise and outliers that affect individual models and produce more stable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e855d-8d16-43b1-a19d-b9629488626c",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75662285-c443-4764-8637-0cef269c8aa8",
   "metadata": {},
   "source": [
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique in machine learning that involves building multiple models using subsets of the training data and combining their predictions. The idea behind bagging is to reduce the variance of a single model by averaging the predictions of multiple models built on different samples of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7d44d-c0dc-422d-a16f-9aff23306e35",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b0c2b-2e93-420c-971a-bd2f2a60ea7b",
   "metadata": {},
   "source": [
    "\n",
    "Boosting is an ensemble technique in machine learning that involves building multiple weak models sequentially, where each model learns from the mistakes of its predecessor, with the aim of creating a strong final model. The idea behind boosting is to combine weak models to create a powerful ensemble that performs better than any single model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729085bd-9e89-440c-aab3-6e29a2466d58",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa35fa2-8eee-499a-a9bc-ce6ff853fe7f",
   "metadata": {},
   "source": [
    "1. Improved accuracy: Ensemble techniques can improve the accuracy of the predictive model by combining the predictions of multiple models. The ensemble can capture more complex patterns in the data and overcome the limitations of individual models.\n",
    "\n",
    "2. Robustness: Ensemble techniques can be more robust to noise and outliers in the data than individual models. The ensemble can filter out the noise and outliers that affect individual models and produce more stable predictions.\n",
    "\n",
    "3. Reduction of overfitting: Ensemble techniques can reduce overfitting by averaging the predictions of multiple models. The ensemble can generalize better to unseen data and avoid overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e8236-958b-4953-8f2c-9fe48fe2b906",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea0bcb-a501-45ce-a6ba-84db79684ebf",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models. There are cases where an individual model may outperform an ensemble, such as when the individual model is already highly accurate and the data is clean and well-behaved.\n",
    "\n",
    "Ensemble techniques can also have some disadvantages, such as increased computational complexity, longer training times, and higher memory requirements, compared to individual models. Moreover, the interpretation of the ensemble results can be more challenging than that of an individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643cb6d-7fee-4a0c-9403-8c78609bab5c",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c27c7-b05e-4acd-8cd3-c437bfdc38cd",
   "metadata": {},
   "source": [
    "The confidence interval calculated using the bootstrap method represents the range of values where the true population parameter is likely to fall with a certain degree of confidence. The bootstrap method is a non-parametric technique that does not make assumptions about the underlying distribution of the data, and therefore can be used in a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db38d0f-f8fd-4121-b3c1-b7c084127101",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba5b1a-e56d-4bad-8784-faeba8109ab2",
   "metadata": {},
   "source": [
    "\n",
    "Bootstrap is a resampling technique used in statistics to estimate the variability of a statistic or to make inference about a population parameter. The bootstrap method works by repeatedly resampling the original data to create a large number of new datasets, from which the statistic of interest is calculated. The bootstrap method can be used for various statistical procedures, such as calculating confidence intervals, hypothesis testing, and model selection.\n",
    "\n",
    "The steps involved in the bootstrap method are as follows:\n",
    "\n",
    "Sample data: Take a random sample of size n (the same as the size of the original dataset) from the population or the original dataset with replacement. This creates a bootstrap sample.\n",
    "\n",
    "Calculate statistic: Calculate the statistic of interest (mean, median, variance, correlation, etc.) on the bootstrap sample.\n",
    "\n",
    "Repeat: Repeat steps 1 and 2 many times (typically 1,000 or more times) to create many bootstrap samples and calculate the statistic of interest for each bootstrap sample.\n",
    "\n",
    "Estimate the standard error: Calculate the standard deviation of the statistic across the bootstrap samples. This is called the standard error of the statistic.\n",
    "\n",
    "Construct confidence intervals: Construct a confidence interval for the statistic of interest by using the standard error and the percentile method. For example, to construct a 95% confidence interval, take the 2.5th and 97.5th percentiles of the bootstrap distribution of the statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04617982-6923-4118-86ef-95206b9d3a1c",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a2b98-b7f9-4bbd-af9e-2c70cafb3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "height <- c(15, 14, 13, 16, 18, 14, 16, 15, 15, 16,\n",
    "            17, 15, 14, 16, 17, 14, 15, 13, 14, 17,\n",
    "            15, 16, 15, 13, 14, 16, 18, 15, 15, 17,\n",
    "            14, 15, 14, 16, 18, 16, 14, 15, 17, 16,\n",
    "            15, 13, 16, 15, 17, 14, 15, 16, 17, 15)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B <- 10000\n",
    "\n",
    "# Create bootstrap samples\n",
    "means <- replicate(B, mean(sample(height, replace = TRUE)))\n",
    "\n",
    "# Calculate standard error\n",
    "se <- sd(means)\n",
    "\n",
    "# Calculate confidence interval\n",
    "ci <- quantile(means, c(0.025, 0.975))\n",
    "\n",
    "# Print results\n",
    "cat(\"Bootstrap estimate of mean height = \", mean(means), \"\\n\")\n",
    "cat(\"Standard error = \", se, \"\\n\")\n",
    "cat(\"95% Confidence interval = [\", ci[1], \", \", ci[2], \"]\")\n",
    "##output\n",
    "Bootstrap estimate of mean height =  15.08814 \n",
    "Standard error =  0.2782667 \n",
    "95% Confidence interval = [ 14.55 ,  15.63 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe6469-079e-4d98-9aa1-b9cd9324dd38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
